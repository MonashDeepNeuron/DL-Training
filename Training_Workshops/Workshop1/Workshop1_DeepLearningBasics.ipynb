{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yA4ptcRlDKy7",
        "tags": []
      },
      "source": [
        "# Workshop 1 - Deep Learning Baics\n",
        "\n",
        "Welcome to your first workshop! This notebook contains skeleton code and useful comments to help you work through the exercise.\n",
        "\n",
        "Feel free to add in your own markdown for additional comments, and also directly comment your code.\n",
        "\n",
        "Gdrive Link to Workshops: https://drive.google.com/drive/folders/1f3T05QC9yBY0w3GUfv1nfDJbleMGzlmg?usp=drive_link\n",
        "\n",
        "GitHub Link to Workshops: https://github.com/MonashDeepNeuron/Dl-Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aaC1kl9ZjTHM"
      },
      "source": [
        "# BEFORE YOU START (GOOGLE COLAB USERS)\n",
        "* **MAKE A COPY IF YOU ARE USING THE GDRIVE LINK OR I SWEAR TO GOD I'LL GIVE YOU THE MOST DISAPPOINTED LOOK AND MAKE YOU VERY SAD**\n",
        "* Click on Runtime -> Change runtime type -> Make sure hardware accelerator is set to GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ijWitb4vDKzI"
      },
      "outputs": [],
      "source": [
        "## Libraries, you do not need to import any additional libraries for this workshop\n",
        "\n",
        "import numpy as np ## Numpy is the fundamental building block of understanding tensors (matrices) within Python\n",
        "import matplotlib.pyplot as plt ## Matplotlib.pyplot is the graphing library that we will be using\n",
        "import random ## Useful for sampling\n",
        "import sys ## Useful to retrieve some system information\n",
        "\n",
        "import math # Basic math library\n",
        "\n",
        "import os ## Useful for running command line within python\n",
        "from IPython.display import Image ## For markdown purposes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5f6-10WDKzK"
      },
      "source": [
        "# Logistic Regression - A Quick Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1cUWLQfVDKzL"
      },
      "source": [
        "The name **Logistic Regression** is slightly misleading: Instead of regression, we are solving **binary classification** problems with this appraoch!\n",
        "\n",
        "More specifically, we model the probabilities of the outcomes of our classification problem, and we can thus understand logistic regression like an extension of linear regression to solve classification (_i.e._, for categorical data).\n",
        "\n",
        "A visual comparison of Linear Regression vs. Logistic regression and especially their 'output' predictions $Y$ for different input points $X$ in a simple 1D case is depicted in the following image:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "id": "LNCgDlGwDKzL",
        "outputId": "e171ab0a-9509-444b-e6b7-c82857a96ce6"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<img src=\"https://miro.medium.com/max/1400/1*dm6ZaX5fuSmuVvM4Ds-vcg.jpeg\" width=\"800\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Image(url='https://miro.medium.com/max/1400/1*dm6ZaX5fuSmuVvM4Ds-vcg.jpeg', width=800)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NV9BuqV0DKzM",
        "tags": []
      },
      "source": [
        "# Section 1 - The Sigmoid function and making predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L5_4RFDTDKzO"
      },
      "source": [
        "In this section, you will be writing the first parts of your code that is essential to predict the outcome of a logistic regression problem. </br>\n",
        "\n",
        "In detail, you are going to\n",
        "- 1.1 Implement and visualise the **sigmoid function**\n",
        "- 1.2 Write code to **predict the outcome** of a classification problem using a pre-trained logistic regression model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1-C6Kt0DKzO",
        "tags": []
      },
      "source": [
        "## 1.1  The sigmoid function\n",
        "\n",
        "The '_sigmoid function_' $\\sigma$, is a mathematical function that shows a characteristic \"S\"-shaped curve as you've seen in the slides. We commonly use this function in our logistic regression to map the regression outputs to a range from 0 to 1. </br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJCHkyYwDKzO",
        "tags": []
      },
      "source": [
        "### Computing the sigmoid\n",
        "In this task, you are now first asked to write a function that computes the output of the sigmoid function $\\sigma(\\boldsymbol{x})$ for any input value $\\boldsymbol{x}$. </br>\n",
        "\n",
        "$$ \\sigma(x) = {1 \\over 1 + e^{-x}} $$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "_0V22c3eDKzO"
      },
      "outputs": [],
      "source": [
        "# Implement the sigmoid function\n",
        "def sigmoid(x):\n",
        "    x = np.clip(x, -500, 500) # this line is used to prevent overflow error as a result of x being too small\n",
        "    return ### YOUR CODE HERE ###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOQLTIGQDKzO"
      },
      "source": [
        "To test the correctness of your implementation, call your sigmoid function with the following input values:\n",
        "- $x_1 = 3.$\n",
        "- $\\boldsymbol{x}_2 = [2., 9.]$\n",
        "- $\\boldsymbol{x}_3 = [ [3., -6.], [4., -8.] ]$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "6pz-NS3DDKzP"
      },
      "outputs": [],
      "source": [
        "## Define the inputs x1, x2 and x3\n",
        "x_1 = 3\n",
        "x_2 = np.array([2,9])\n",
        "x_3 = np.array([[3,-6],[4,-8]])\n",
        "\n",
        "# Use your implemented sigmoid function to obtain the results for the given x_i\n",
        "z_1 = sigmoid(x_1)\n",
        "z_2 = sigmoid(x_2)\n",
        "z_3 = sigmoid(x_3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TrQ_DcP-DKzP",
        "outputId": "4e2509c7-f868-41a5-bc81-ec9a3d71f16c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "z_1: None\n",
            "z_2: None\n",
            "z_3: None\n"
          ]
        }
      ],
      "source": [
        "## Now let's print the obtained results\n",
        "print(f'z_1: {z_1}')\n",
        "print(f'z_2: {z_2}')\n",
        "print(f'z_3: {z_3}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WwCIadtnDKzP"
      },
      "source": [
        "If your implementation is correct, you should obtain a printed output similar to this: </br>\n",
        "- z_1: &nbsp;&nbsp;0.9525741268224334 </br>\n",
        "- z_2: [0.88079708 0.99987661] </br>\n",
        "- z_3: [[9.52574127e-01 2.47262316e-03]</br>\n",
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
        "[9.82013790e-01 3.35350130e-04]]</br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ieX8OaLeDKzQ",
        "tags": []
      },
      "source": [
        "### Visualising your Sigmoid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "oduE10NVDKzQ",
        "outputId": "12435d2b-ca9b-4843-f5e3-99a02efdf081"
      },
      "outputs": [],
      "source": [
        "## Visualise the output of the sigmoid function in a range from -10 to 10\n",
        "x = np.arange(-10, 10, 0.4) # 50 data points only\n",
        "sigma_x = sigmoid(x)\n",
        "\n",
        "# plotting the Sigmoid function with red scatter plot\n",
        "plt.scatter(x,sigma_x,c='r')\n",
        "plt.axvline(0.0, color='k') # zero-crossing\n",
        "plt.ylim(-0.1, 1.1) # outputs of sigmoid do not exceed (0,1)\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('$\\sigma (x)$')\n",
        "plt.title('Sigmoid Visualisation')\n",
        "\n",
        "# y axis ticks and gridline\n",
        "plt.yticks([0.0, 0.5, 1.0])\n",
        "ax = plt.gca()\n",
        "ax.yaxis.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpwEzhK9jTHR"
      },
      "source": [
        "#### Q. What would happen if you only used a small number of datapoints to plot?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "DDqCqR1qDKzQ",
        "tags": []
      },
      "outputs": [],
      "source": [
        "## Answer here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4cSMzcLDKzR"
      },
      "source": [
        "## 1.2 Making predictions\n",
        "You will now use your implemented sigmoid function to solve an actual classification problem using logistic regression. </br>\n",
        "\n",
        "A prediction $\\hat{y}$ can be obtained by using our logistic regression model via $\\hat{y}=\\sigma(\\boldsymbol{w}^\\top \\boldsymbol{x})$\n",
        "\n",
        "Why? From the image below convince yourself this is the case :)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 620
        },
        "id": "-GGiZ0WajTHS",
        "outputId": "c315881e-1cd9-44ba-94e6-0c962f42ca2d"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<img src=\"https://miro.medium.com/v2/resize:fit:1170/1*-hdeb1xx3Fl4-L74hO41OQ.png\" width=\"800\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Image(url='https://miro.medium.com/v2/resize:fit:1170/1*-hdeb1xx3Fl4-L74hO41OQ.png', width=800)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "fUtoj_NjDKzR"
      },
      "outputs": [],
      "source": [
        "# Read in data.npz using np.load --> data has been saved via np.savez\n",
        "\n",
        "# Components can be accessed like a dictionary after the file has been loaded, and the file contains the following:\n",
        "# 'X_train' : training data we're going to use\n",
        "# 'y_train' : labels for the training data\n",
        "# 'X_test'  : test data we're going to use for evaluation, but NOT for training\n",
        "# 'y_test'  : labels for the test data\n",
        "# 'w_pret'  : a set of random weights for the logistic regression model\n",
        "\n",
        "data = ### YOUR CODE HERE ###\n",
        "X_train = ### YOUR CODE HERE ###\n",
        "y_train = ### YOUR CODE HERE ###\n",
        "X_test = ### YOUR CODE HERE ###\n",
        "y_test = ### YOUR CODE HERE ###\n",
        "w_pret = ### YOUR CODE HERE ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 456
        },
        "id": "eANoBycnDKzR",
        "outputId": "f4bd5e71-db83-4647-a6b7-c52e9351c336"
      },
      "outputs": [],
      "source": [
        "# Add side by side plots here to visualise your train and test data.\n",
        "# This is the two classes you will be classifying.\n",
        "train_class_0 = np.where(y_train == 0)\n",
        "train_class_1 = np.where(y_train == 1)\n",
        "test_class_0 = np.where(y_test == 0)\n",
        "test_class_1 = np.where(y_test == 1)\n",
        "\n",
        "fig = plt.figure(figsize=(10,5))\n",
        "ax1 = fig.add_subplot(1, 2, 1)\n",
        "ax1.scatter(X_train[train_class_0,0], X_train[train_class_0,1], c='b', label=\"Class 0\")\n",
        "ax1.scatter(X_train[train_class_1,0], X_train[train_class_1,1], c='r', label=\"Class 1\")\n",
        "ax1.set_xlabel('X_train[0]')\n",
        "ax1.set_ylabel('X_train[1]')\n",
        "ax1.set_title('Training data')\n",
        "ax1.legend()\n",
        "\n",
        "ax2 = fig.add_subplot(1, 2, 2)\n",
        "ax2.scatter(X_test[test_class_0,0], X_test[test_class_0,1], c='b', label=\"Class 0\")\n",
        "ax2.scatter(X_test[test_class_1,0], X_test[test_class_1,1], c='r', label=\"Class 1\")\n",
        "ax2.set_xlabel('X_test[0]')\n",
        "ax2.set_ylabel('X_test[1]')\n",
        "ax2.set_title('Test data')\n",
        "ax2.legend()\n",
        "\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rvn0ktnNDKzR",
        "outputId": "94e2d72a-81bd-4bf8-9a19-6d005699c0dc"
      },
      "outputs": [],
      "source": [
        "## Check the shape of the data using .shape (e.g X_train.shape)\n",
        "# Note that we assume certain shapes of data for the basic logistic regression formulas to work,\n",
        "# so make sure you understand which elements should be multiplied with each other!\n",
        "# Hint: In case the data is stored in a different shape, you can easily transpose the matrices!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nnM0-tnojTHT"
      },
      "source": [
        "#### Q. Explain the shapes of the data. What do the numbers represent, and why do they differ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "sfwHom57DKzS"
      },
      "outputs": [],
      "source": [
        "## Answer here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SUN32skODKzS"
      },
      "source": [
        "### Predicting class probabilities via logistic regression\n",
        "After having obtained the data and a set of weights for our logistic regression model, you are now going to:\n",
        "- Implement a function to predict outcomes using a linear regression model (taking in data $\\boldsymbol{X}$ and parameters $\\boldsymbol{w}$)\n",
        "- Test your function on the example data provided below\n",
        "- Test your function on the read-in training data $\\boldsymbol{X}_{train}$\n",
        "\n",
        "Note that our data is stored as [number of samples, dim], so you need to pay attention to possibly required transpose operations to perform the calculations correctly. </br>\n",
        "\n",
        "We also want the same to be true for our predictions, _i.e._ return them in the format [number of samples, 1] to match the input data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "D73OdKw0DKzS"
      },
      "outputs": [],
      "source": [
        "# Write a prediction function -> We predict the output class probability, NOT the class label (no 0,1 rounding)\n",
        "def predict(X, w):\n",
        "    # Remember y = sigmoid(w.T * x)\n",
        "    # For matrix multiplication, the inner dimensions must be the same! [AxB] * [BxC]\n",
        "    \n",
        "    # Perfrom Matrix multiplication\n",
        "    w_t = ### YOUR CODE HERE ### # Hint: Use np.transpose()\n",
        "    Z = ### YOUR CODE HERE ### # Hint: Use np.matmul()\n",
        "\n",
        "    # Calling Sigmoid\n",
        "    y_hat = ### YOUR CODE HERE ###\n",
        "\n",
        "    return ### YOUR CODE HERE### # Hint: Make sure y_hat matches the shape of y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4TXhoasPDKzT"
      },
      "source": [
        "Test your prediction function using the following toy data points / samples:\n",
        "- $\\boldsymbol{X}_1 = [[0.5, 0.1]]$\n",
        "- $\\boldsymbol{X}_2 = [ [-0.5, -0.7], [0.4, 0.2] ]$\n",
        "- $\\boldsymbol{X}_3 = [ [-0.3, -0.15], [0.89, -0.02], [-0.35, 0.01], [0.26, -0.64] ]$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oAbNLu5BDKzT",
        "outputId": "1a598609-206a-4898-a7c2-2b2bc1c1efab"
      },
      "outputs": [],
      "source": [
        "## Define the toy input data\n",
        "X_1 = np.array([[0.50,0.1]])\n",
        "X_2 = np.array([[-0.5,-0.7],[0.4,0.2]])\n",
        "X_3 = np.array([[-0.3,-0.15],[0.89,-0.02],[-0.35,0.01],[0.26,-0.64]])\n",
        "\n",
        "## Obtain predictions using predict function and pretrained parameters w_pret\n",
        "### YOUR CODE HERE ###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0LrGhpIIDKzT"
      },
      "source": [
        "**If your predict function works as intended, your results should be close to:** </br>\n",
        "y_hat_1: &nbsp;[[0.51370692]]</br>\n",
        "y_hat_2: [[0.61837619]</br>\n",
        "&emsp;&emsp;&emsp;&emsp; &nbsp;&nbsp;[0.48409849]]</br>\n",
        "y_hat_3: [[0.51192789]</br>\n",
        "&emsp;&emsp;&emsp;&emsp;&nbsp;&nbsp;  [0.56831845]</br>\n",
        "&emsp;&emsp;&emsp;&emsp;&nbsp;&nbsp;  [0.47251372]</br>\n",
        "&emsp;&emsp;&emsp;&emsp;&nbsp;&nbsp;  [0.65665833]]</br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFw-ywbODKzU"
      },
      "source": [
        "Now let's additionally test the predict function on the $\\boldsymbol{X}_{train}$ data we read in from the stored file and compare the output shape to the shape of the provided labels $\\boldsymbol{y}_{train}$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k9Zx-3rvDKzU",
        "outputId": "ed8503db-f873-42a4-f6f1-52d193f0d34c"
      },
      "outputs": [],
      "source": [
        "## Test on the read-in X_train data:\n",
        "y_hat_data = ### YOUR CODE HERE ###\n",
        "\n",
        "# Compare shapes:\n",
        "print(y_hat_data.shape)\n",
        "print(y_train.shape)\n",
        "\n",
        "# check first predicted element\n",
        "### YOUR CODE HERE ###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZnM69N1DKzU"
      },
      "source": [
        "If your predict function works correctly, the shapes of the predictions and the provided labels should match. </br>\n",
        "\n",
        "You can also take a look at the first element of the prediction - it should have a value of around $0.98278992$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LOVcKaI-DKzV"
      },
      "source": [
        "# Section 2 - Training a model via Gradient Descent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stmORewfDKzV"
      },
      "source": [
        "In this second task, you will be writing code for the essential components to **train your own logistic model via Gradient Descent** given some training data. </br>\n",
        "\n",
        "In detail, you are going to\n",
        "- 2.1 Implement a function that computes and returns **gradient and cost** of the logistic regression\n",
        "- 2.2 Write code to perform the actual **gradient descent algorithm** for a fixed number of iterations and **train your own logistic regression model** given some training data\n",
        "- 2.3 **Evaluate your model** on previously unseen test data points\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7760MIpDKzV"
      },
      "source": [
        "### Cross Entropy Loss\n",
        "We commonly use the so-called _Cross Entropy_ Loss to calculate the cost of our logistic regression problem. This loss function can be defined as </br>\n",
        "</br>\n",
        "\\begin{equation}\n",
        "    \\mathcal{L}_{\\mathrm{CE}}(\\boldsymbol{w})= - \\frac{1}{m}\\sum_{i=1}^{m} \\Big\\lbrace y_i \\log \\Big(\\underbrace{\\sigma\\left(\\boldsymbol{w}^\\top \\boldsymbol{x}_i\\right)}_{\\hat{y}_i}\\Big) + \\left( 1 - y_i \\right) \\log \\Big( 1- \\underbrace{\\sigma\\left(\\boldsymbol{w}^\\top \\boldsymbol{x}_i\\right)}_{\\hat{y}_i}\\Big) \\Big\\rbrace\n",
        "\\end{equation}\n",
        "</br>\n",
        "In this notation, $\\sigma(z)$ denotes the **sigmoid** function, and $(\\boldsymbol{x}_1,y_1),(\\boldsymbol{x}_2,y_2),\\dots,(\\boldsymbol{x}_m,y_m)$ with $\\boldsymbol{x}_i \\in \\mathbb{R}^n, y_i \\in \\lbrace 0, 1\\rbrace$ represent the $m$ training samples (with labels $y_i$). </br>\n",
        "\n",
        "The gradient of the cross entropy loss w.r.t. the weights $\\boldsymbol{w}$ can be written as\n",
        "</br>\n",
        "\n",
        "\\begin{equation}\n",
        "    \\nabla_{\\boldsymbol{w}}\\mathcal{L}_{\\mathrm{CE}} = \\frac{1}{m}\\sum_{i=1}^{m} \\Big(\\underbrace{\\sigma\\left(\\boldsymbol{w}^\\top \\boldsymbol{x}_i\\right)}_{\\hat{y}_i} - y_i \\Big) \\boldsymbol{x}_i\n",
        "\\end{equation}\n",
        "</br>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 541
        },
        "id": "A88pO7PPjTHV",
        "outputId": "e479a472-f748-4c56-dec6-1c8b379d1d0e"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<img src=\"https://raw.githubusercontent.com/shruti-jadon/Data_Science_Images/main/cross_entropy.png\" width=\"800\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Image(url='https://raw.githubusercontent.com/shruti-jadon/Data_Science_Images/main/cross_entropy.png', width=800)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IsEiCOOCjTHV"
      },
      "source": [
        "Cross Entropy (CE) Loss is used in binary classification problems as it allows us to measure the dissimilarity between predicted probabilities and the actual binary label for that datapoint.\n",
        "\n",
        "It has some useful fetures:\n",
        "* CE loss uses logaritmic terms, which amplify the loss for misclassified predictions, particularly for predictions with high confidence (e.g $\\hat{y}_i$ is 0.1, but $y$ is actually 1)\n",
        "    * As a result, it penalises the model greatly for wrong predictions\n",
        "* CE loss is continuous and differentiable, allowing us to use gradient descent for model optimization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7_gBEFSDKzV"
      },
      "source": [
        "## 2.1  Gradient and Cost Computation\n",
        "In this part, we want to define a function that is able to compute our cross-entropy loss $\\mathcal{L}_{\\mathrm{CE}}$, as well as the gradient $\\nabla_{\\boldsymbol{w}}\\mathcal{L}_{\\mathrm{CE}}$ of our loss $\\mathcal{L}_{\\mathrm{CE}}$ _w.r.t._ the parameters $\\boldsymbol{w}$. </br>\n",
        "\n",
        "As you can see above, all we need to compute the gradient vector is the prediction of the model $\\hat{y}$ and the actual labels $y$, as well as the input data points $\\boldsymbol{X}$. The loss itself is even more simple and only requires the predictions $\\hat{y}$ and true labels $y$.\n",
        "\n",
        "Note that we take the mean $1 \\over m $ for both the loss and its gradient."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "6wOEvAMdDKzV"
      },
      "outputs": [],
      "source": [
        "def compute_loss_and_grad(X, y, y_hat):\n",
        "    # Inputs:\n",
        "    #    Set of samples X (each sample is a row in X)\n",
        "    #    Corresponding ground-truth labels y\n",
        "    #    Predicted class probabilities y_hat\n",
        "\n",
        "    # Import smallest number represented to handle log(0) edge case\n",
        "    eps = 1e-12\n",
        "\n",
        "    # Compute the mean cross-entropy loss w.r.t. the parameters w (make sure to take the mean)\n",
        "    loss = ### YOUR CODE HERE ###\n",
        "    # log(0) might throw error, so handled via small eps\n",
        "\n",
        "    # Compute the gradient vector (mean over all samples)\n",
        "    grad_vec = ### YOUR CODE HERE ###\n",
        "\n",
        "    # Return loss and gradient vector\n",
        "    return loss, grad_vec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQWXrUHvDKzV"
      },
      "source": [
        "## 2.2 Training with Gradient Descent\n",
        "\n",
        "Gradient descent is a popular first-order iterative optimisation method that has become ubiquitous in the machine and deep learning context. The idea is to find the local minimum of a differentiable function by repeatedly taking steps in the opposite direction of the gradient of the function at the current point - i.e. in the direction of its steepest descent.\n",
        "\n",
        "The main parts of the algorithm work as follows:\n",
        "- Initialise hyperparameters like step-size aka learning rate, and number of iterations\n",
        "- Randomly initialise the set of parameters $\\boldsymbol{w}_{init}$ that shall be optimised\n",
        "- For a certain number of iterations, do:\n",
        "    - Obtain the prediction using the current weights $\\boldsymbol{w}_i$ and training data $\\boldsymbol{X}_{train}$\n",
        "    - Compute the loss $\\mathcal{L}_{\\mathrm{CE}}$ and the gradient vector $\\nabla_{\\boldsymbol{w}}\\mathcal{L}_{\\mathrm{CE}}$ w.r.t. the current parameters $\\boldsymbol{w}_i$\n",
        "    - Update the parameters using the gradient vector and learning rate _lr_\n",
        "- After all iterations are finished, return the final optimised set of parameters\n",
        "\n",
        "In addition, we ask you to also:\n",
        "- Return a list of all losses (one value for each iteration)\n",
        "- Return a list of all gradient vectors (one vector for each iteration)\n",
        "- Implement an option via the argument \"logging\" to switch on printing a string containing the 'iteration' and the 'loss' for each iteration\n",
        "\n",
        "Note that the initial set of parameters $\\boldsymbol{w}_{init}$, the hyperparameters as well as the training data $\\boldsymbol{X}_{train}$ and labels $\\boldsymbol{y}_{train}$ are passed as input arguments to your function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "UfXHy9PQDKzV"
      },
      "outputs": [],
      "source": [
        "## Setting some hyperparameters:\n",
        "lr = 0.5         # Learning rate\n",
        "num_epochs = 20    # Number of iterations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "G9SRckUqDKzW"
      },
      "outputs": [],
      "source": [
        "def gradient_descent(w_init, num_epochs, lr, X_train, y_train, logging=False):\n",
        "    #  Create empty lists to store the values for loss and gradient vector over all\n",
        "    #  'num_epochs' iterations of our gradient descent optimisation procedure\n",
        "    losses = []\n",
        "    grad_vecs = []\n",
        "\n",
        "    # Init the parameters\n",
        "    w = w_init\n",
        "\n",
        "    ## Implement the actual gradient descent using the previously implemented functions\n",
        "    # For each epoch:\n",
        "    #   Pass our input data X into our model (w parameters) to get a set of predictions\n",
        "    #   Compute the loss for this set of predictions, and its gradient\n",
        "    #   Use the gradient of the loss to perform a gradient descent optimization step for the w parameters\n",
        "    #   Log the computed loss and gradient in the loss/grad_vecs list\n",
        "\n",
        "    ### YOUR CODE HERE ###\n",
        "\n",
        "    return w, losses, grad_vecs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "kRDNf20uDKzW"
      },
      "outputs": [],
      "source": [
        "## Run the function on the training set\n",
        "# Start from a random initialisation\n",
        "np.random.seed(12345)\n",
        "w_init = np.random.randn(X_train.shape[1],1)\n",
        "\n",
        "# Obtain the final weights via gradient descent\n",
        "w_final, _, _ = gradient_descent(w_init, num_epochs, lr, X_train, y_train, logging=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "moGGqXf2DKzW"
      },
      "source": [
        "## 2.3 Evaluating the trained model\n",
        "\n",
        "After you have obtained your optimised set of parameters $\\boldsymbol{w}^{*}$, let's see how your model performs! </br>\n",
        "\n",
        "To this end, you are going to:\n",
        "- Obtain the predictions (class probabilities) $\\hat{y}_{train}$ for the training data $\\boldsymbol{X}_{train}$ using $\\boldsymbol{w}^{*}$\n",
        "- Obtain the predictions (class probabilities) $\\hat{y}_{test}$ for the test data $\\boldsymbol{X}_{test}$ using $\\boldsymbol{w}^{*}$\n",
        "- Convert these into the actual predicted labels (everything with probability >=0.5 is more likely to be of class 1 and thus gets label '1' assigned ; below gets label '0')\n",
        "- Count how many samples have been correctly classified and compute the percentage (_i.e._, the accuracy in %)\n",
        "- Report your obtained accuracies for both training and test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3JhvkLMnDKzW",
        "outputId": "5c5a7853-7db1-4793-9ccb-93f86d83e7f8"
      },
      "outputs": [],
      "source": [
        "## Evaluate the obtained model on training data and previously unseen test data\n",
        "# Obtain predicted class probabilities for train and test data\n",
        "y_hat_train = predict(X_train,w_final)\n",
        "y_hat_test  = predict(X_test,w_final)\n",
        "\n",
        "# Obtain actual class labels (everything >=0.5 is class1, rest class0)\n",
        "c_hat_train = np.where(y_hat_train >= 0.5, 1, 0)\n",
        "c_hat_test = np.where(y_hat_test >= 0.5, 1, 0)\n",
        "\n",
        "# Evaluate the classification accuracy for training and test data\n",
        "acc_train = np.sum(np.equal(c_hat_train, y_train))/y_train.shape[0]\n",
        "acc_test = np.sum(np.equal(c_hat_test, y_test))/y_test.shape[0]\n",
        "\n",
        "# Print outputs\n",
        "print(f'Training accuracy: {acc_train:.3f} | Test accuracy: {acc_test:.3f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vFc2QrY3DKzW"
      },
      "source": [
        "If your implementation work correctly and using the provided hyperparameter settings, you should obtain something around:\n",
        "\n",
        "Training accuracy: 0.868 | Test accuracy: 0.853"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Roc0yoXXDKzW"
      },
      "source": [
        "# Section 3 - Analysing convergence and accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4W2eLONDKzX"
      },
      "source": [
        "## 3.1 Improving the accuracy\n",
        "Our previous choice of hyperparameters might not be the best possible one (or even close to it). </br>\n",
        "Can you achieve a **better test accuracy** by changing the hyperparameters from the previous task? </br>\n",
        "Try to improve upon the standard choice by varying the learning rate `lr` and number of training iterations `num_epochs`. Report your choice and best results below!\n",
        "\n",
        "**Potentially also let them analyse training vs test accuracy? Would it overfit on this data?**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "38jfdY6CDKzX",
        "outputId": "7a7ede00-dc5d-4ceb-e7aa-644c5f253c75"
      },
      "outputs": [],
      "source": [
        "## Report your best results as well as hyperparameter choices!\n",
        "def analyseLogisticRegression(w_init, num_epochs, lr, X_train, y_train):\n",
        "    w_final, _, _ = gradient_descent(w_init, num_epochs, lr, X_train, y_train, logging=False)\n",
        "\n",
        "    # Obtain predicted class probabilities for train and test data\n",
        "    y_hat_train = predict(X_train,w_final)\n",
        "    y_hat_test  = predict(X_test,w_final)\n",
        "\n",
        "    # Obtain actual class labels (everything >=0.5 is class1, rest class0)\n",
        "    c_hat_train = np.where(y_hat_train >= 0.5, 1, 0)\n",
        "    c_hat_test = np.where(y_hat_test >= 0.5, 1, 0)\n",
        "\n",
        "    # Evaluate the classification accuracy for training and test data\n",
        "    acc_train = np.sum(np.equal(c_hat_train, y_train))/y_train.shape[0]\n",
        "    acc_test = np.sum(np.equal(c_hat_test, y_test))/y_test.shape[0]\n",
        "\n",
        "    # Print outputs\n",
        "    print(f'Training accuracy: {acc_train:.3f} | Test accuracy: {acc_test:.3f} | LR: {lr} | Epochs: {num_epochs}')\n",
        "\n",
        "# Choose a hyperparameter to vary e.g learning rate, num_epochs\n",
        "# Call analyseLogisticRegression() for these new hyperparameters, making sure to start from a random w_init seeded by np.random.seed(12345)\n",
        "\n",
        "### YOUR CODE HERE ###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6mm18iypDKzX"
      },
      "source": [
        "## 3.2 Training convergence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4G9DCekDKzX"
      },
      "source": [
        "In this section, you are now going to take a closer look at how gradient descent 'progresses' for different choices of hyperparameters.\n",
        "\n",
        "Given the provided set of learning rates _lrs_, run your implemented gradient descent method and plot the obtained loss values over the number of iterations for each learning rate.  </br>\n",
        "\n",
        "Additionally save the training and test accuracies achieved for each learning rate. </br>\n",
        "\n",
        "You can re-use/copy-and-paste your code from above, or define it as a function!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "P0kPNv-eDKzX"
      },
      "outputs": [],
      "source": [
        "## Provided list of learning rates to train on:\n",
        "lrs = [0.05, 0.1, 0.5, 1.0, 2.5, 5., 10., 75.]\n",
        "# Max number of iterations for GD algorithm to run\n",
        "num_epochs = 20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 490
        },
        "id": "4ua_PDNsDKzY",
        "outputId": "37b86570-811e-4ec0-bd8b-3550047b90a9"
      },
      "outputs": [],
      "source": [
        "## Run gradient descent for all learning rates, and plot results\n",
        "fig, ax = plt.subplots()\n",
        "x_vals = range(num_epochs)\n",
        "leg = []  # legend entries\n",
        "w_finals = {}\n",
        "for lr in lrs:\n",
        "    ## Start from a random initialisation\n",
        "    np.random.seed(12345)\n",
        "    w_init = np.random.randn(X_train.shape[1],1)\n",
        "\n",
        "    # Obtain the final weights via gradient descent\n",
        "    w_final, losses, _ = gradient_descent(w_init, num_epochs, lr, X_train, y_train, logging=False)\n",
        "    ax.plot(x_vals, losses, label=f'{lr}')\n",
        "    leg.append(f'lr = {lr}')\n",
        "    w_finals[f'lr={lr}'] = w_final\n",
        "\n",
        "# Add legend, title and label the axes\n",
        "ax.set_title(\"Convergence of Gradient Descent for Various Learning Rates\")\n",
        "ax.set_ylabel(\"Loss\")\n",
        "ax.set_xlabel(\"Current Epoch/Iteration\")\n",
        "ax.legend(loc=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8bsrw1GaDKzY",
        "outputId": "f040b8dd-3734-4fcf-da78-ef64bf6a02be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " >>> Training accuracies for different learning rates: <<<\n",
            "lr=0.05: \t 0.845\n",
            "lr=0.1: \t 0.863\n",
            "lr=0.5: \t 0.868\n",
            "lr=1.0: \t 0.855\n",
            "lr=2.5: \t 0.838\n",
            "lr=5.0: \t 0.822\n",
            "lr=10.0: \t 0.833\n",
            "lr=75.0: \t 0.827\n",
            "\n",
            " >>> Test accuracies for different learning rates: <<<\n",
            "lr=0.05: \t 0.822\n",
            "lr=0.1: \t 0.84\n",
            "lr=0.5: \t 0.852\n",
            "lr=1.0: \t 0.83\n",
            "lr=2.5: \t 0.815\n",
            "lr=5.0: \t 0.798\n",
            "lr=10.0: \t 0.812\n",
            "lr=75.0: \t 0.8\n"
          ]
        }
      ],
      "source": [
        "## Evaluating the stored parameter sets to retrieve train and test accuracies\n",
        "def evaluate(X,y,w):\n",
        "    # Obtain predicted class probabilities\n",
        "    y_hat = predict(X,w)\n",
        "\n",
        "    # Obtain actual class labels (everything >=0.5 is class1, rest class0)\n",
        "    c_hat = np.where(y_hat >= 0.5, 1, 0)\n",
        "\n",
        "    # Evaluate the classification accuracy\n",
        "    acc = np.sum(np.equal(c_hat, y))/y.shape[0]\n",
        "\n",
        "    return acc\n",
        "\n",
        "print(' >>> Training accuracies for different learning rates: <<<')\n",
        "for k,v in w_finals.items():\n",
        "    print(f'{k}: \\t {round(evaluate(X_train, y_train, v),3)}')\n",
        "\n",
        "print('\\n >>> Test accuracies for different learning rates: <<<')\n",
        "for k,v in w_finals.items():\n",
        "    print(f'{k}: \\t {round(evaluate(X_test, y_test, v), 3)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rj3kDkwvDKzY"
      },
      "source": [
        "## 3.3 Describe & Explain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tffDy4PIDKzY"
      },
      "source": [
        "Answer the following questions and elaborate on your observations:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "-JP6vW07DKzZ"
      },
      "outputs": [],
      "source": [
        "## Answer the following questions:\n",
        "\n",
        "# What do you observe? Are there general trends in convergence visible, and are they ‘good’ or ‘bad’?\n",
        "   \n",
        "# What do you think would be the best choice out of the set of provided learning rates, and why?\n",
        "\n",
        "# Can you find an even better one?\n",
        "\n",
        "# How is the accuracy on the training and test data related to these convergence results?\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "interpreter": {
      "hash": "70931b1f795e9f6c4a65c63751d6e1a5ce2513e077f66f45128f19b7bedfe461"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
