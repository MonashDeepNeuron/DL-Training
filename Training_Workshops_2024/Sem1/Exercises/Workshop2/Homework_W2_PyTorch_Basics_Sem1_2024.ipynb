{"cells":[{"cell_type":"markdown","metadata":{"id":"pGTnInyEharX"},"source":["# Before you start\n","1. **Don't edit this file, make a copy first:**\n","  * Click on File -> Save a copy in Drive\n","\n","2. Also do the following:\n","  * Click on Runtime -> Change runtime type -> Make sure hardware accelerator is set to GPU"]},{"cell_type":"markdown","metadata":{"id":"cknZbgNdR6IM"},"source":["# An Overview Before We Begin\n","Here's a couple of important concepts to note down before we start:\n","- There is no one particular methodology that works best in all scenarios. This includes everything from model architecture, learning rate, loss function, and optimizer.\n","- Like any other engineering project, validation of what we have built is just as important as building it."]},{"cell_type":"markdown","metadata":{"id":"ZueWb5ZRDeyT"},"source":["# Library Imports"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DlJ_aMz20rxE"},"outputs":[],"source":["import torch\n","from torch import nn\n","from torch import optim\n","from torchvision import datasets, transforms, models\n","\n","from tqdm.notebook import tqdm"]},{"cell_type":"markdown","metadata":{"id":"hepIxB_VKggb"},"source":["# Defining Path Variables"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hO3kSpVxKiqC"},"outputs":[],"source":["train_path = 'data/train'\n","valid_path = 'data/valid'"]},{"cell_type":"markdown","metadata":{"id":"zszDeZ7CJ0Z3"},"source":["# Creating DataLoader\n","- Creating a Dataset. This bundles the data in a way that the model can understand.\n","- Creating a DataLoader(wraps an iterable around the dataset). This tells the model how to receive the images. Including batch_size, num_workers, shuffle configurations, etc."]},{"cell_type":"markdown","metadata":{"id":"R1LivjuALGLi"},"source":["## [Instantiating Transforms](https://https://pytorch.org/docs/stable/torchvision/transforms.html)\n","\n","  - the transforms.Normalize([...]) function basically changes the data slightly according to the average RGB weights in the dataset.\n","  - This may seem a bit strange to you, why do we do this? Turns out, normalizing the data before training results in noticeable performance gains and reduction in training time. Since we generalize the data which makes it easier for the model to train\n","  - So why does normalizing data have such performance boosts in training? That's because by itself, the RGB values of the raw data have differing ranges. the blue pixel may have a range of of 0->125 while the red pixel may have a range of 120->245. This different range often causes headaches for the optimizer and it takes the gradient descent to converge much slowly as it has to cater to the differing conditions of both the red and blue pixel.\n","  - What batch normalization does is that it makes the RGB ranges somewhat similar, so the optimizer doesn't have such a hard time trying to cater for all the different ranges, and thus gradient descent covnerges faster.\n","  - More information here https://medium.com/@urvashilluniya/why-data-normalization-is-necessary-for-machine-learning-models-681b65a05029\n","\n","Now, in the below block of code, come up with a set of tranforms for the training and validation datasets that you think might be suitable. Try using transforms such as rotation, flipping, normalizing, etc.\n","\n","Find the documentation for the transforms here : https://pytorch.org/vision/stable/transforms.html"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mXzCC_fKJzVT"},"outputs":[],"source":["# Define transforms for the training and validation set\n","training_transforms = transforms.Compose([...,       # Insert random rotation, 30 degrees\n","                                          ...,   # Insert random horizontal flip\n","                                          ...,               # Convert to tensor\n","                                          transforms.Normalize([0.485, 0.456, 0.406],\n","                                                               [0.229, 0.224, 0.225])])\n","\n","validation_transforms = transforms.Compose([...,       # Insert random rotation, 30 degrees\n","                                            ...,   # Insert random horizontal flip\n","                                            ...,\n","                                            transforms.Normalize([0.485, 0.456, 0.406],\n","                                                                 [0.229, 0.224, 0.225])])"]},{"cell_type":"markdown","metadata":{"id":"qQ9qAGv1LgLC"},"source":["## [Torchvision datasets](https://pytorch.org/docs/stable/torchvision/datasets.html)\n","There are a number of ways to create a dataset, for example:\n","- Use an available Torchvision dataset\n","    - We're using one below called CIFAR10 which we used in the first training session\n","- Use ImageFolder to create a dataset from folders\n","- Write your own dataset as a subclass of torch.utils.data.Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"l_7ENLBwKYKO","outputId":"68deb7e1-31e7-41f7-a361-c06e5c06f7f8","executionInfo":{"status":"ok","timestamp":1694272752922,"user_tz":-600,"elapsed":13491,"user":{"displayName":"Pham Nguyen","userId":"06242186830021996900"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to data/train/cifar-10-python.tar.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 170498071/170498071 [00:03<00:00, 43701989.63it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting data/train/cifar-10-python.tar.gz to data/train\n","Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to data/valid/cifar-10-python.tar.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 170498071/170498071 [00:03<00:00, 43959885.92it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting data/valid/cifar-10-python.tar.gz to data/valid\n"]}],"source":["#TODO: Create a Dataset Object\n","training_dataset = datasets.CIFAR10(train_path, train=..., transform=..., download=True)\n","validation_dataset = datasets.CIFAR10(valid_path, train=..., transform=..., download=True)"]},{"cell_type":"markdown","metadata":{"id":"njZjCupaUJQN"},"source":["## [Instantiating DataLoader](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html)\n","- This tells the model *how* how to receive the data; with the dataset as an input, along with batch size and shuffle as args\n","\n","Now code up data loaders for the training and validation datasets as per the followig specs :\n","\n","- In the training_loader, we're telling it batch_size = 32, and we want to shuffle the dataloader after each epoch.\n","- In the validation_loader, we also take batch_size = 32 but we DON'T want to shuffle the dataloader. This is because we want to be testing on the data in the same order to make sure the model really is improving and didn't hit a fluke ordering of the dataset."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NFxxs8XJULmT"},"outputs":[],"source":["# TODO: Create DataLoader class for training and validation set, with batch_size\n","# 32 and set the correct shuffle parameters\n","from torch.utils.data import DataLoader\n","training_loader = DataLoader(..., batch_size=32, shuffle=True)\n","validation_loader = DataLoader(..., batch_size=32, shuffle=False)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3t0Ga6RYLOA4","outputId":"dc6ce371-942d-4431-e263-f58eeb5ffc7c","executionInfo":{"status":"ok","timestamp":1694273130843,"user_tz":-600,"elapsed":339,"user":{"displayName":"Pham Nguyen","userId":"06242186830021996900"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(['airplane',\n","  'automobile',\n","  'bird',\n","  'cat',\n","  'deer',\n","  'dog',\n","  'frog',\n","  'horse',\n","  'ship',\n","  'truck'],\n"," ['airplane',\n","  'automobile',\n","  'bird',\n","  'cat',\n","  'deer',\n","  'dog',\n","  'frog',\n","  'horse',\n","  'ship',\n","  'truck'])"]},"metadata":{},"execution_count":6}],"source":["# Check what classes are in our dataset\n","\n","training_dataset.classes, validation_dataset.classes\n","\n","#mane note of how many classes are there"]},{"cell_type":"markdown","metadata":{"id":"3NwqQ0DMFlBC"},"source":["# Instantiating ResNet18\n","- In PyTorch when a model is downloaded, you need to reconfigure the 'classification' layer as the pretrained model that was trained for ImageNet, hence it comes ready to classify for many classes (we only need it to classify 10 classes)\n","- In addition, downloaded models from PyTorch come unfrozen, which means we need to 'freeze' the entire network except for the classification layer so we can perform the first batch of training.\n","- Unfrozen here means that the weights in each layer can be updated during training and in some cases this is not what we want since the pretrained model has been optimized in such a way that it could classify images in the Imagenet dataset with a high accuracy"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":141},"id":"VXc2SykWNm8J","outputId":"2fe92d22-e80d-4977-f9dd-9d15abe72681","executionInfo":{"status":"error","timestamp":1713781258263,"user_tz":-600,"elapsed":1153,"user":{"displayName":"Pham Nguyen","userId":"06242186830021996900"}}},"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'models' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-f42d3b1a6bb5>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresnet18\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResNet18_Weights\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDEFAULT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'models' is not defined"]}],"source":["model = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)"]},{"cell_type":"markdown","metadata":{"id":"iMqHGCoyGb0d"},"source":["## Freezing The Model\n","- *%%capture* is colab syntax, it essentially stops the cell from printing out any logs. This is purely for *aesthetic* purposes.\n","- We're looping through all the parameters in the model, and setting requires_grad = False. Which 'freezes' the entire model.\n","  - requires_grad stands for 'requires gradient'. When requires_grad is False, it's weights does not get updated and hence it is 'frozen'."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oy-Sty6TXRWt"},"outputs":[],"source":["%%capture\n","# TODO: freeze the model\n","..."]},{"cell_type":"markdown","metadata":{"id":"SiySi5PyGzKF"},"source":["## Reconfiguring The Classification Layer\n","- model.fc = the last layer of the network\n","- model.fc.in_features = the features going into the last layer\n","- But we know that the pretrained Resnet18 is designed for dozens of classes, but we only need it to classify 10 classes, so we're going to have to replace the last layer.\n","  - To do so, we use nn.Linear(out_ftrs, 10).\n","  - This way, we keep the same numnber of features going into the last layer, but only change the number of features going out, which in this case is 10.\n","  - We then reinsert it to the model using model.fc = nn.Linear(out_ftrs, 10)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pw-6SGpx0vKJ","outputId":"1361f55f-7df1-46e8-b07a-3f491e590d3f","executionInfo":{"status":"ok","timestamp":1694273212324,"user_tz":-600,"elapsed":333,"user":{"displayName":"Pham Nguyen","userId":"06242186830021996900"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["Linear(in_features=512, out_features=1000, bias=True)"]},"metadata":{},"execution_count":11}],"source":["# Print last layer of the model\n","model.fc"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R-X4os5mPCMj"},"outputs":[],"source":["# TODO: Change the last layer of model\n","\n","model.fc = nn.Linear(..., ...) # Redefine last linear layer of network to output 10 classes"]},{"cell_type":"markdown","metadata":{"id":"_jKFQfhkhGf1"},"source":["# The Training Function\n","- The fit() function in Pytorch is used to 'fit' the model and train it\n","<br>\n","<br>\n","\n","Everytime we run through a 'batch' of data we need to do a few things\n","1. Clear the gradients from the previous loop  \n","2. Perform a forward pass (put the input through the model once)\n","3. Calculate the loss\n","4. Back propogate the loss\n","5. Update the parameter weights by taking a step with the optimiser"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gwvXsvTnhEKd"},"outputs":[],"source":["# Function for the training\n","\n","def train(model, train_loader, loss_fn, optimizer, device):\n","    ... # puts the model in training mode\n","    running_loss = 0\n","    with tqdm(total=len(train_loader)) as pbar:\n","        for i, data in enumerate(train_loader, 0): # loops through training data\n","            inputs, labels = ... # separate inputs and labels (outputs),\n","            inputs, labels = ... # puts the data on the GPU\n","\n","            # forward + backward + optimize\n","            optimizer... # clear the gradients in model parameters\n","            outputs = ... # forward pass and get predictions, how do we get outputs?\n","            loss = loss_fn(outputs, labels) # calculate loss, how do we get loss\n","            loss... # calculates gradient w.r.t to loss for all parameters in model that have requires_grad=True\n","            optimizer... # iterate over all parameters in the model with requires_grad=True and update their weights.\n","\n","            running_loss += loss.item() # sum total loss in current epoch for print later\n","\n","            pbar.update(1) #increment our progress bar\n","\n","    return running_loss/len(train_loader) # returns the total training loss for the epoch"]},{"cell_type":"markdown","metadata":{"id":"ZPD3ezm9OZ8t"},"source":["# The Validation Function\n","- A validation function is essential in any model training, because it helps you validate how well your model is performing on the validation dataset.\n","\n","Note: the validation function validates the model performance by passing the entire validation set through the model ONCE. Also note that we cacluate the loss but don't propogate it back or update any weights!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h2QY58AvqJDI"},"outputs":[],"source":["# Function for the validation pass\n","\n","def validation(model, val_loader, loss_fn, device):\n","    model.eval() # puts the model in validation mode\n","    running_loss = 0\n","    total = 0\n","    correct = 0\n","\n","    with torch.no_grad(): # save memory by not saving gradients which we don't need\n","        with tqdm(total=len(val_loader)) as pbar:\n","            for images, labels in iter(val_loader):\n","                images, labels = ... # put the data on the GPU\n","                outputs = ... # passes image to the model, and gets a ouput which is the class probability prediction\n","\n","                val_loss = ... # calculates val_loss from model predictions and true labels\n","                running_loss += val_loss.item()\n","                _, predicted = torch.max(outputs, 1) # turns class probability predictions to class labels\n","                total += labels.size(0) # sums the number of predictions\n","                correct += (predicted == labels).sum().item() # sums the number of correct predictions\n","\n","                pbar.update(1)\n","\n","        return running_loss/len(val_loader), correct/total # return loss value, accuracy"]},{"cell_type":"markdown","metadata":{"id":"cb8hexwc-8ES"},"source":["#Things to note about our training and validation functions\n","\n","## What's the difference between `model.train()` and `model.eval()`?\n","These two are extremely important to your training and validation loops. `model.eval()` takes away some layers that should only be used during training such as dropout and batch normalisation. It's important to always use `model.train()` when training and `model.eval()` when evaluating.\n","\n","## Why do we need torch.no_grad()?\n","Running `with torch.no_grad()` means that we don't want gradients which is what happens during validation or testing, we don't need to update any gradients so we don't need to record them. Running this means that we optimize our code to not do things it doesn't need to.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"jLUTOwwH4-9T"},"source":["# Setting Up Training\n","- When training models, it is substantially faster to train on NVIDIA GPU's, beacuse they offer a parallel computing platform called [cuda](https://developer.nvidia.com/cuda-zone) (cudnn is the API package to interface with cuda) that speeds up these computations exponentially.\n","- So here we check if cuda is available with cuda.is_available().\n","  - Following which, we send the model to the cuda device so the computation can be done on the GPU."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"On6Xdb9TqNOF"},"outputs":[],"source":["%%capture\n","import torch.backends.cudnn as cudnn\n","torch.cuda.empty_cache()\n","cudnn.benchmark = True  # Optimise for hardware\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device) # send model to GPU"]},{"cell_type":"markdown","metadata":{"id":"q4pZVkP5QiwM"},"source":["# Loss Function & Optimizers\n","- The **Loss Function** calculates how 'far' the model's class probability predictions are to the actual labels.\n","  - Notice how I'm saying \"how far the model's class prob predictions are to the actual labels\" instead of \"how innacurate the model is\", that's because accurate/inaccurate is the percentage of correctly or incorrectly predicted labels. This may sound the same to you but just keep this in mind, it will all make sense in due time.\n","  - CrossEntropyLoss is a way of calculating the loss of a model, other loss functions include Kullback Leibler Divergence Loss, Sparse Multiclass Cross-Entropy Loss, and much more.\n","\n","- **The Optimizer** is a way of updating the weights of the model to minimize loss. In other words, the optimizer is the part of deep learning that helps a model 'learn'.\n","- In this case we're using the Adam optimizer, this is purely by random choice as no particular optimizer can be said to be superior to the other. There's an important concept in deep learning called \"no free lunch\", which means there isn't a particular methodology that will achieve the best outcome for all scenarios, what it comes down to is experimentation.\n","  - a lr of 0.001 is also chosen, this is usually a good learning rate start from with the Adam optimizer, however to get a more optimum learning rate, experimentation would need to be done. (The Pytorch documentation includes defaults for each different optimizer)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IoIZ_7nsqKbt"},"outputs":[],"source":["loss_fn = ... # again, what loss function would be good here?\n","optimizer = ... # Adam optimizer with lr=0.001"]},{"cell_type":"markdown","metadata":{"id":"xedFUaJCMNMR"},"source":["# Let The Training Begin! Part 1\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":422,"referenced_widgets":["cc4aae4fd2ed48aca194469cc700005b","810ece190ebf43ec9cd11d3d0c39efe1","b4c93d768f5e423ab2d3ebe73b5899ad","6669f838cf25446381dcfa1d00b710d8","dc5bbf61658c4756b906f3dc75b7eb83","5cbf745650394c20adf628ff0e25cefa","6b77c3da69d8449d96db385d77d345fb","f6fa9982d154429eacf2988ee6ca460f","381e38154bf242d48306d7ed03c1a6e2","6e7750125ad4427f94e42bf89a719e32","ac5e65e08a9f4db98000a53a941898c8"]},"id":"lL8voSz-4fKi","outputId":"2064a99f-bf95-4803-b617-81b2153caac9","executionInfo":{"status":"error","timestamp":1694273490980,"user_tz":-600,"elapsed":19478,"user":{"displayName":"Pham Nguyen","userId":"06242186830021996900"}}},"outputs":[{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/1563 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cc4aae4fd2ed48aca194469cc700005b"}},"metadata":{}},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-19-063790c8dff3>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtotal_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m \u001b[0;31m#feel free to play around with how many epochs you guys wanna do\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# loops through number of epochs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# train the model for one epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# after training for one epoch, run the validation() function to see how the model is doing on the validation dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Epoch: {}/{}, Training Loss: {}, Val Loss: {}, Val Accuracy: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-13-6be873fd6080>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, loss_fn, optimizer, device)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mrunning_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpbar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# loops through training data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m             \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;31m# separate inputs and labels (outputs),\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# puts the data on the GPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 633\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    634\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 677\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    678\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/cifar.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1494\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1495\u001b[0;31m         \u001b[0mforward_call\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1496\u001b[0m         \u001b[0;31m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1497\u001b[0m         \u001b[0;31m# this function, and just call forward.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["total_epoch = 5 #feel free to play around with how many epochs you guys wanna do\n","for epoch in range(total_epoch): # loops through number of epochs\n","  train_loss = train(model, training_loader, loss_fn, optimizer, device)  # train the model for one epoch\n","  val_loss, accuracy = validation(model, validation_loader, loss_fn, device) # after training for one epoch, run the validation() function to see how the model is doing on the validation dataset\n","  print(\"Epoch: {}/{}, Training Loss: {}, Val Loss: {}, Val Accuracy: {}\".format(epoch+1, total_epoch, train_loss, val_loss, accuracy))\n","  print('-' * 20)\n","\n","print(\"Finished Training\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9-J2_a9Gbggo"},"outputs":[],"source":["# We then save the model so we can come back later to it if need be\n","torch.save(model.state_dict(), 'stage-1')"]},{"cell_type":"markdown","metadata":{"id":"kbNvgW6rOGLg"},"source":["# Let The Training Begin! Part 2\n","- Function below allows the rest of the model to be optimized for this specific task.\n","- The cell after is exactly the same as the training of the model in 'let The Training Begin! Part 1', just that we're retraining for 2 epochs."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H7rN2kYE2zLW"},"outputs":[],"source":["# TODO: Unfreeze the model\n","..."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Hwo7c3hz22lr"},"outputs":[],"source":["total_epoch = 10\n","for epoch in range(total_epoch): # loops through number of epochs\n","  train_loss = train(model, training_loader, loss_fn, optimizer, device) # train the model for one epoch\n","  val_loss, accuracy = validation(model, validation_loader, loss_fn, device) # after training for one epoch, run the validation() function to see how the model is doing on the validation dataset\n","  print(\"Epoch: {}/{}, Training Loss: {}, Val Loss: {}, Val Accuracy: {}\".format(epoch+1, total_epoch, train_loss, val_loss, accuracy))\n","  print('-' * 20)\n","\n","print(\"Finished Training\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZikgH7ANmcXA"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"widgets":{"application/vnd.jupyter.widget-state+json":{"cc4aae4fd2ed48aca194469cc700005b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_810ece190ebf43ec9cd11d3d0c39efe1","IPY_MODEL_b4c93d768f5e423ab2d3ebe73b5899ad","IPY_MODEL_6669f838cf25446381dcfa1d00b710d8"],"layout":"IPY_MODEL_dc5bbf61658c4756b906f3dc75b7eb83"}},"810ece190ebf43ec9cd11d3d0c39efe1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5cbf745650394c20adf628ff0e25cefa","placeholder":"​","style":"IPY_MODEL_6b77c3da69d8449d96db385d77d345fb","value":" 62%"}},"b4c93d768f5e423ab2d3ebe73b5899ad":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"danger","description":"","description_tooltip":null,"layout":"IPY_MODEL_f6fa9982d154429eacf2988ee6ca460f","max":1563,"min":0,"orientation":"horizontal","style":"IPY_MODEL_381e38154bf242d48306d7ed03c1a6e2","value":972}},"6669f838cf25446381dcfa1d00b710d8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6e7750125ad4427f94e42bf89a719e32","placeholder":"​","style":"IPY_MODEL_ac5e65e08a9f4db98000a53a941898c8","value":" 972/1563 [00:19&lt;00:10, 54.96it/s]"}},"dc5bbf61658c4756b906f3dc75b7eb83":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5cbf745650394c20adf628ff0e25cefa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6b77c3da69d8449d96db385d77d345fb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f6fa9982d154429eacf2988ee6ca460f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"381e38154bf242d48306d7ed03c1a6e2":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6e7750125ad4427f94e42bf89a719e32":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ac5e65e08a9f4db98000a53a941898c8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}